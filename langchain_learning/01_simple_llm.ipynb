{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API key from the environment\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    model = ChatMistralAI(name=\"open-mixtral-8x22b\")\n",
    "else:\n",
    "    print(\"API key not found in environment or .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Answer briefly!\"),\n",
    "    HumanMessage(content=\"What is indexing in database?\"),\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Indexing in a database is a technique used to optimize the performance of a database by reducing the time it takes to retrieve data. It is similar to the index found at the back of a book, which allows you to quickly find specific information without having to scan every page. In a database, an index is a data structure that stores a small portion of the data in a table, along with a pointer to the location of the full record. This allows the database to quickly locate and retrieve specific records based on the indexed data, improving search and query performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(parser.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Give exact math formula for provided term\"),\n",
    "    HumanMessage(content=\"Self-attention in transformer architecture\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The self-attention mechanism in the Transformer architecture can be calculated using the following formulas:\n",
       "\n",
       "First, the input sequence `x` is represented as a matrix `X` with dimensions `(batch_size, sequence_length, embedding_dimension)`.\n",
       "\n",
       "Then, the query, key, and value matrices `Q`, `K`, and `V` are computed as follows:\n",
       "\n",
       "Q = X \\* W^Q\n",
       "K = X \\* W^K\n",
       "V = X \\* W^V\n",
       "\n",
       "where `W^Q`, `W^K`, and `W^V` are learnable weight matrices with dimensions `(embedding_dimension, heads * head_dimension)`, and `heads` is the number of attention heads.\n",
       "\n",
       "Next, the attention scores `A` are calculated as:\n",
       "\n",
       "A = softmax(Q \\* K^T / sqrt(head_dimension))\n",
       "\n",
       "where `K^T` is the transpose of the key matrix, `sqrt` is the square root function, and `softmax` is the row-wise softmax function.\n",
       "\n",
       "Finally, the output of the self-attention layer `O` is computed as:\n",
       "\n",
       "O = A \\* V\n",
       "\n",
       "where `*` denotes matrix multiplication.\n",
       "\n",
       "Therefore, the self-attention mechanism in the Transformer architecture can be summarized as:\n",
       "\n",
       "O = softmax(Q \\* K^T / sqrt(head\\_dimension)) \\* V\n",
       "\n",
       "where `Q`, `K`, and `V` are computed from the input sequence matrix `X` using learnable weight matrices `W^Q`, `W^K`, and `W^V`. The output `O` has the same shape as the input `X`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into Russian:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Machine Learning is good', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt_template.invoke({\"language\": \"Russian\", \"text\": \"Machine Learning is good\"})\n",
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Машинное обучение хорошо (Mashinnoe obuchenie horosho).\n",
       "\n",
       "I hope this helps! Let me know if you have any other questions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = chain.invoke({\"language\": \"Russian\", \"text\": \"Machine Learning is good\"})\n",
    "Markdown(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-to-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
