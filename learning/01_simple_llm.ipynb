{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API key from the environment\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    model = ChatMistralAI(name=\"open-mixtral-8x22b\")\n",
    "else:\n",
    "    print(\"API key not found in environment or .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Answer briefly!\"),\n",
    "    HumanMessage(content=\"What is indexing in database?\"),\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Indexing in a database is a technique used to optimize the performance of a database by reducing the time it takes to retrieve data. It is similar to the index found at the back of a book, which allows you to quickly locate specific information without having to scan every page.\n",
       "\n",
       "In a database, an index is a data structure that is created on one or more columns of a table to enable fast searching and data retrieval. Indexes can significantly improve the performance of database queries, especially for large databases, by reducing the amount of disk I/O and CPU time required to locate and retrieve data. However, creating and maintaining indexes can also add some overhead to the database, so it is important to use them judiciously."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(parser.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Give exact math formula for provided term\"),\n",
    "    HumanMessage(content=\"Self-attention in transformer architecture\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The self-attention mechanism in the Transformer architecture can be calculated using the following formulas:\n",
       "\n",
       "1. Query, Key, and Value matrices are first calculated by multiplying the input matrix (X) with the corresponding weight matrices (W^Q, W^K, W^V):\n",
       "\n",
       "Query (Q) = X \\* W^Q\n",
       "Key (K) = X \\* W^K\n",
       "Value (V) = X \\* W^V\n",
       "\n",
       "where * denotes matrix multiplication.\n",
       "\n",
       "2. The attention scores are calculated by taking the dot product of the Query and Key matrices, dividing by the square root of the Key dimension (d_k), and applying a softmax function:\n",
       "\n",
       "Attention(Q, K, V) = softmax(Q \\* K^T / sqrt(d_k)) \\* V\n",
       "\n",
       "where ^T denotes the transpose of a matrix.\n",
       "\n",
       "3. The output of the self-attention mechanism is then calculated by adding a bias term (b) and applying a weight matrix (W^O) through linear transformation:\n",
       "\n",
       "Output = Attention(Q, K, V) \\* W^O + b\n",
       "\n",
       "where + denotes matrix addition.\n",
       "\n",
       "These formulas are used to calculate the self-attention mechanism in each of the attention heads in the Transformer architecture. The final output is then calculated by concatenating the outputs of all attention heads and applying another linear transformation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into Russian:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Machine Learning is good', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt_template.invoke({\"language\": \"Russian\", \"text\": \"Machine Learning is good\"})\n",
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Машинное обучение хорошо (Mashinnoe obuchenie horosho).\n",
       "\n",
       "I hope this helps! Let me know if you have any other questions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = chain.invoke({\"language\": \"Russian\", \"text\": \"Machine Learning is good\"})\n",
    "Markdown(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-to-llm-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
